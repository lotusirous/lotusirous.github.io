<!DOCTYPE html>
<html class="no-js" lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Kha's - 카</title>
	<link rel="stylesheet" href="/css/vs.min.css">
<script src="/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="Install bare-metal kubernetes cluster for production.">
	<meta name="generator" content="Hugo 0.56.3" />
	<meta property="og:title" content="Install bare-metal kubernetes cluster" />
<meta property="og:description" content="Install bare-metal kubernetes cluster for production." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/install-kubernetes/" />
<meta property="article:published_time" content="2018-04-16T00:00:00+00:00" />
<meta property="article:modified_time" content="2018-04-16T00:00:00+00:00" />

	
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">
	<link rel="stylesheet" href="/css/style.css">
	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container">
		<div class="logo">
			<a class="logo__link" href="/" title="Kha&#39;s" rel="home">
				<div class="logo__title">Kha&#39;s</div>
				<div class="logo__tagline">A goal is a dream with a deadline</div>
			</a>
		</div>
		<div class="divider"></div>
	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Install bare-metal kubernetes cluster</h1>
			<div class="post__meta meta">
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg>
	<time class="meta__text" datetime="2018-04-16T00:00:00">April 16, 2018</time>
</div>

<div class="meta__item-categories meta__item">
	<svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2l1 2h8v11h-16v-13z"/></svg>
	<span class="meta__text"><a class="meta__link" href="/categories/development" rel="category">Development</a></span>
</div>
</div>
		</header><div class="content post__content clearfix">
			<p>Kubernetes is a orchestration for containers these day with a well-suited design in distributed system. In order to kick off your journey, this article provides a guide to install and troubleshoot a bare metal production-ready kubernetes cluster. A cluster is a set of tightly connected computers that work together to create a powerful computing system. The cluster in this guide contains three minimal CentOS virtual machines with 1 kubernetes master and 2 nodes.</p>

<blockquote>
<p>For whom who want to run kubernetes cluster locally for testing, it is better to use <a href="https://kubernetes.io/docs/tasks/tools/install-minikube/">minikube</a> or <a href="https://microk8s.io/">microk8s</a>.</p>
</blockquote>

<p>The following is requirements for our setup guide:</p>

<ol>
<li><p>3 virtual machines (VMWare or VirtualBox) have <code>/etc/hosts</code> as follows:</p>

<pre><code class="language-txt">192.168.233.160 master.local
192.168.233.162 node2.local
192.168.233.161 node1.local
</code></pre></li>

<li><p>2 GB or more of RAM per machine. 2 CPUs or more on the master.</p></li>

<li><p>High-speed network connection between machines.</p></li>
</ol>

<h2 id="installation">Installation</h2>

<p>Since Kubernetes is a container orchestration, it includes container and orchestration. Containers are a solution to the problem of reliably when environment is inconsistent (move from developer laptop to production environment). On the other hand, orchestration is all about managing the lifecycles of containers dynamically.</p>

<h3 id="step-1-how-can-i-install-a-container">Step 1. How can I install a container?</h3>

<p>In this guide, I used docker container, the <a href="https://docs.docker.com/install/linux/docker-ce/centos/">original document</a> describes very detail installation steps. Make sure you configured <a href="https://docs.docker.com/install/linux/linux-postinstall/">post installation</a> to execute <code>docker</code> command for normal user.</p>

<p>In case of, the following command does not work. You just <strong><em>logout and re-login</em></strong> to the system.</p>

<pre><code class="language-sh">docker run hello-world
</code></pre>

<h3 id="step-2-install-kubernetes-cluster">Step 2. Install kubernetes cluster</h3>

<p>By default, Docker have a built-in orchestration named <em>Swarm</em>. A comparison of
Kubernetes vs Docker Swarm describes <a href="https://platform9.com/blog/kubernetes-docker-swarm-compared/">here</a></p>

<p>Networking is a heart of our cluster since container have to start/stop and scale as a independent unit in system. Each unit have a limit of resource which is a fundamental for resource management. Kubernetes takes advantage of container networking - <em>iptables</em> and system resource management <em>cgroup</em> (control groups) to achieved it purpose.</p>

<blockquote>
<p>If you make any failure. Using the command <code>kubeadm reset</code> to rollback.</p>
</blockquote>

<p><strong>Step 2.1 Configure forwarding for bridge</strong></p>

<p>For example, <code>flannel</code> requires enable forwarding <code>iptables</code>.</p>

<pre><code class="language-sh">cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl --system
</code></pre>

<p><strong>Step 2.2 Configure <code>cgroup</code> driver</strong></p>

<p><em>kubelet</em> and <em>dockerd</em> use <code>cgroup</code> to control system resources — such as CPU time, system memory, network bandwidth, or combinations of these resource.
Thus, it have to configure as the same group on all nodes in our cluster. If <code>cgroupfs</code> of  <code>kubelet</code> and <code>dockers</code>  are mismatch, <code>kube-api</code> will not start. We verify <code>kubelet</code> service cgroup variable matches <code>docker</code> cgroup:</p>

<pre><code class="language-sh">docker info | grep -i cgroup
cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
</code></pre>

<p>If the Docker <code>cgroup</code> driver and the kubelet config is mismatched, execute the following command to change the kubelet <code>systemd</code> variable Docker <code>cgroup</code> driver.</p>

<pre><code class="language-sh">sed -i &quot;s/cgroup-driver=systemd/cgroup-driver=cgroupfs/g&quot; /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
</code></pre>

<p>Reload kubelet</p>

<pre><code>systemctl daemon-reload
</code></pre>

<p><code>cgroupfs</code> of  <code>kubelet</code> and <code>dockers</code>  are mismatch:</p>

<pre><code>[init] This might take a minute or longer if the control plane images have to be pulled.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz' failed with error: Get http://localhost:10255/healthz: dial tcp 127.0.0.1:10255: getsockopt: connection refused.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz' failed with error: Get http://localhost:10255/healthz: dial tcp 127.0.0.1:10255: getsockopt: connection refused.
[kubelet-check] It seems like the kubelet isn't running or healthy.
[kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10255/healthz' failed with error: Get http://localhost:10255/healthz: dial tcp 127.0.0.1:10255: getsockopt: connection refused.
</code></pre>

<p><strong>Step 2.3</strong></p>

<p>Now, we&rsquo;re use template <code>kubeadm</code> to create a cluster with flannel networking on <strong>master node</strong>. This step takes quite long since it pulls container images from Internet. This step follows flannel <a href="https://github.com/coreos/flannel/blob/master/Documentation/troubleshooting.md">troubleshooting guide </a></p>

<p><strong>On MASTER NODES</strong></p>

<p>We&rsquo;re gonna initialize our cluster in master using the following command:</p>

<pre><code>kubeadm init --pod-network-cidr=10.244.0.0/16
</code></pre>

<p>The result as following:</p>

<pre><code class="language-txt">To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of machines by running the following on each node
as root:

  kubeadm join 192.168.233.160:6443 --token s969hf.io2noks78xq7u1hz --discovery-token-ca-cert-hash sha256:f9ccaa60fb8a599b0b9a120d5e94d615441929e7d753d2426c3d88b94b94291
</code></pre>

<p>Make sure you execute command quickly because this file is NOT persistent. Your token will difference from the token in this guide.</p>

<p><strong>Step 2.3 Copy configuration for client</strong></p>

<p>By default, a <code>kubectl</code> loads configuration in <code>$HOME/.kube</code> with user&rsquo;s permission.</p>

<pre><code>mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
</code></pre>

<p><strong>Step 2.4 Join cluster in WORKER NODES</strong></p>

<p>Using the token generated from master as follows:</p>

<pre><code>kubeadm join 192.168.233.160:6443 --token s969hf.io2noks78xq7u1hz --discovery-token-ca-cert-hash sha256:f9ccaa60fb8a599b0b9a120d5e94d615441929e7d753d2426c3d88b94b94291
</code></pre>

<p>The result will be</p>

<pre><code>[preflight] Starting the kubelet service
[discovery] Trying to connect to API Server &quot;192.168.233.160:6443&quot;
[discovery] Created cluster-info discovery client, requesting info from &quot;https://192.168.233.160:6443&quot;
[discovery] Requesting info from &quot;https://192.168.233.160:6443&quot; again to validate TLS against the pinned public key
[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server &quot;192.168.233.160:6443&quot;
[discovery] Successfully established connection with API Server &quot;192.168.233.160:6443&quot;

This node has joined the cluster:
* Certificate signing request was sent to master and a response
  was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the master to see this node join the cluster.
</code></pre>

<p>After this step, you can shows nodes status in our cluster. Execute the following command on <strong>MASTER NODE</strong>.</p>

<pre><code>kubectl get nodes
</code></pre>

<p>The certs are mismatched between <code>kubectl</code> and <code>kube-api</code> server</p>

<pre><code>$ kubectl get nodes 
Unable to connect to the server: x509: certificate signed by unknown authority (possibly because of &quot;crypto/rsa: verification error&quot; while trying to verify candidate authority certificate &quot;kubernetes&quot;`
</code></pre>

<p><strong><em>Solution</em></strong> You should <a href="#copy-configuration-for-users-such-as-user-or-root">copy the <code>.kube</code> directory step</a>.</p>

<p>However, all nodes are in <code>NOT READY</code> status because we have not applied <code>CNI</code> yet.</p>

<p>!!! warning
    If <code>/var/log/message</code> shows the following content</p>

<pre><code>```
May 28 01:33:51 master kubelet: W0528 01:33:51.662498 1818 cni.go:171] Unable to update cni config: No networks found in /etc/cni/net.d 
May 28 01:33:51 master kubelet: E0528 01:33:51.662990 1818 kubelet.go:2130] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized 
May 28 01:33:56 master kubelet: W0528 01:33:56.664136 1818 cni.go:171] Unable to update cni config: No networks found in /etc/cni/net.d 
May 28 01:33:56 master kubelet: E0528 01:33:56.664284 1818 kubelet.go:2130] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized 
May 28 01:34:01 master kubelet: W0528 01:34:01.665735 1818 cni.go:171] Unable to update cni config: No networks found in /etc/cni/net.d 
May 28 01:34:01 master kubelet: E0528 01:34:01.666054 1818 kubelet.go:2130] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized 
May 28 01:34:06 master kubelet: W0528 01:34:06.668387 1818 cni.go:171] Unable to update cni config: No networks found in /etc/cni/net.d 
May 28 01:34:06 master kubelet: E0528 01:34:06.668622 1818 kubelet.go:2130] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized 
May 28 01:34:11 master kubelet: W0528 01:34:11.670561 1818 cni.go:171] Unable to update cni config: No networks found in /etc/cni/net.d 
May 28 01:34:11 master kubelet: E0528 01:34:11.670688 1818 kubelet.go:2130] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized 
May 28 01:34:16 master kubelet: W0528 01:34:16.671984 1818 cni.go:171] Unable to update cni config: No networks found in /etc/cni/net.d 
May 28 01:34:16 master kubelet: E0528 01:34:16.672069 1818 kubelet.go:2130] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialize
```

This log shows that we are missing a container network controller. However, after worker node join our cluster, we will apply `flannel` networking to all cluster.
</code></pre>

<h4 id="step-2-5-apply-cni-on-master-node">Step 2.5 Apply CNI on MASTER NODE</h4>

<p>we apply flannel networking configuration.</p>

<pre><code>$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml
</code></pre>

<p>If <code>kubectl</code> configuration is corrected, you are able to apply flannel networks as follow.</p>

<pre><code>[test@master ~]$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml
clusterrole.rbac.authorization.k8s.io &quot;flannel&quot; created
clusterrolebinding.rbac.authorization.k8s.io &quot;flannel&quot; created
serviceaccount &quot;flannel&quot; created
configmap &quot;kube-flannel-cfg&quot; created
daemonset.extensions &quot;kube-flannel-ds&quot; created
</code></pre>

<p>After a few seconds, flannel network is applied to our cluters. Make sure, kubeadm generates network configuration file in <code>/etc/cni/net.d/10-flannel.conflist</code></p>

<pre><code class="language-sh">[test@master ~]$ cat /etc/cni/net.d/10-flannel.conflist
{
  &quot;name&quot;: &quot;cbr0&quot;,
  &quot;plugins&quot;: [
    {
      &quot;type&quot;: &quot;flannel&quot;,
      &quot;delegate&quot;: {
        &quot;hairpinMode&quot;: true,
        &quot;isDefaultGateway&quot;: true
      }
    },
    {
      &quot;type&quot;: &quot;portmap&quot;,
      &quot;capabilities&quot;: {
        &quot;portMappings&quot;: true
      }
    }
  ]
}
</code></pre>

<p>Content <code>/var/log/message</code> as follows:</p>

<pre><code>May 28 01:47:08 master kubelet: W0528 01:47:08.732684   18831 cni.go:171] Unable to update cni config: No networks found in /etc/cni/net.d
May 28 01:47:08 master kubelet: E0528 01:47:08.732795   18831 kubelet.go:2130] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
May 28 01:47:12 master dockerd: time=&quot;2018-05-28T01:47:12-04:00&quot; level=info msg=&quot;shim docker-containerd-shim started&quot; address=&quot;/containerd-shim/moby/66a81a53aa98418189e11aabeb66fe849b42c31f62ec93c430a0b94a5d7887ea/shim.sock&quot; debug=false module=&quot;containerd/tasks&quot; pid=20386
May 28 01:47:12 master dockerd: time=&quot;2018-05-28T01:47:12-04:00&quot; level=info msg=&quot;shim reaped&quot; id=66a81a53aa98418189e11aabeb66fe849b42c31f62ec93c430a0b94a5d7887ea module=&quot;containerd/tasks&quot;
May 28 01:47:12 master dockerd: time=&quot;2018-05-28T01:47:12.642103106-04:00&quot; level=info msg=&quot;ignoring event&quot; module=libcontainerd namespace=moby topic=/tasks/delete type=&quot;*events.TaskDelete&quot;
May 28 01:47:13 master kubelet: I0528 01:47:13.871314   18831 kuberuntime_manager.go:513] Container {Name:kube-flannel Image:quay.io/coreos/flannel:v0.10.0-amd64 Command:[/opt/bin/flanneld] Args:[--ip-masq --kube-subnet-mgr] WorkingDir: Ports:[] EnvFrom:[] Env:[{Name:POD_NAME Value: ValueFrom:&amp;EnvVarSource{FieldRef:&amp;ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {Name:POD_NAMESPACE Value: ValueFrom:&amp;EnvVarSource{FieldRef:&amp;ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}}] Resources:{Limits:map[memory:{i:{value:52428800 scale:0} d:{Dec:&lt;nil&gt;} s:50Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:&lt;nil&gt;} s:100m Format:DecimalSI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:&lt;nil&gt;} s:100m Format:DecimalSI} memory:{i:{value:52428800 scale:0} d:{Dec:&lt;nil&gt;} s:50Mi Format:BinarySI}]} VolumeMounts:[{Name:run ReadOnly:false MountPath:/run SubPath: MountPropagation:&lt;nil&gt;} {Name:flannel-cfg ReadOnly:false MountPath:/etc/kube-flannel/ SubPath: MountPropagation:&lt;nil&gt;} {Name:flannel-token-8kbb6 ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:&lt;nil&gt;}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:&amp;SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
May 28 01:47:13 master dockerd: time=&quot;2018-05-28T01:47:13-04:00&quot; level=info msg=&quot;shim docker-containerd-shim started&quot; address=&quot;/containerd-shim/moby/30cb8c7fd16044ac319548c6a6a6751e8c5e54fd81cf33f455825aabde7f60ca/shim.sock&quot; debug=false module=&quot;containerd/tasks&quot; pid=20462
May 28 01:47:15 master NetworkManager[678]: &lt;info&gt;  [1527486435.9543] manager: (flannel.1): new Vxlan device (/org/freedesktop/NetworkManager/Devices/7)
May 28 01:47:15 master NetworkManager[678]: &lt;info&gt;  [1527486435.9618] device (flannel.1): state change: unmanaged -&gt; unavailable (reason 'connection-assumed', sys-iface-state: 'external')
May 28 01:47:15 master NetworkManager[678]: &lt;info&gt;  [1527486435.9640] device (flannel.1): state change: unavailable -&gt; disconnected (reason 'none', sys-iface-state: 'external')
</code></pre>

<h3 id="step-3-verify-results">Step 3. Verify results</h3>

<ul>
<li><p>[ ] Running nodes</p>

<pre><code>[test@master ~]$ kubectl get nodes
NAME           STATUS    ROLES     AGE       VERSION
master.local   Ready     master    8m        v1.10.3
node1.local    Ready     &lt;none&gt;    8m        v1.10.3
node2.local    Ready     &lt;none&gt;    8m        v1.10.3
</code></pre></li>

<li><p>[ ] <strong>Make sure container images is enough and correct tag version in case of offline installation</strong></p>

<pre><code>[root@master test]# docker images -a
</code></pre></li>
</ul>

<h3 id="run-hello-world-deployment">Run hello world deployment</h3>

<p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/">K8s nodejs hello-world deployment</a></p>

<pre><code>kubectl run hello-world --replicas=2 --image=gcr.io/google-samples/node-hello:1.0  --port=8080
</code></pre>

<p>Display information about the Deployment</p>

<pre><code>kubectl get deployments hello-world
kubectl describe deployments hello-world
</code></pre>

<p>Expose as NodePort</p>

<pre><code>kubectl expose deployment hello-world --type=NodePort --name=hello-svc
</code></pre>

<p>view application expose port</p>

<pre><code>kubectl get svc
</code></pre>

<h2 id="troubleshooting">Troubleshooting</h2>

<p>This section, I collected some issues when install kubernetes cluster which is useful for debugging and understand the system.</p>

<h3 id="file-system">File system</h3>

<p>The <a href="https://docs.docker.com/install/linux/docker-ce/centos/#install-docker-ce">docker&rsquo;s official guide</a> recommended <code>overlay2</code> storage driver. However, the backing file system is not always support <code>overlay2</code> <a href="https://docs.docker.com/storage/storagedriver/overlayfs-driver/">file system requirements</a></p>

<p>In RHEL 7.2 uses <code>xfs</code> with <code>ftype=0</code>, you should format the <code>xfs</code> filesystem correctly with the flag <code>-n ftype=1</code></p>

<p>From RHEL 7.5, file system uses default option <code>ftype=1</code></p>

<h3 id="default-route">Default route</h3>

<p>A machine can connect to Internet. In offline-mode, it should have a default route. <code>ip route list</code></p>

<h3 id="disable-security-features">Disable security features</h3>

<ul>
<li>Disable SELinux by setting value <code>SELINUX=disabled</code> in <code>/etc/selinux/config</code></li>

<li><p>Disable permanent swap for kubernetes scheduler optimizes resources. Remove swap partition at boot by commenting swap partition at boot in <code>/etc/fstab</code> or run-time machine by <code>swapoff -a</code></p>

<pre><code class="language-py">print(&quot;hello world&quot;)
</code></pre></li>
</ul>

<h3 id="allow-ports-on-firewall">Allow ports on firewall</h3>

<ul>
<li><p>Turn off (<code>firewalld</code> or <code>iptables</code>) using the commands</p>

<pre><code class="language-txt">sudo systemctl stop firewalld &amp;&amp; sudo systemctl disable firewalld
</code></pre></li>

<li><p>It is a best practice to allow the following port on firewall</p></li>
</ul>

<p><strong>Master nodes</strong></p>

<table>
<thead>
<tr>
<th align="left">Protocol</th>
<th align="left">Direction</th>
<th align="left">Port Range</th>
<th align="left">Purpose</th>
</tr>
</thead>

<tbody>
<tr>
<td align="left">TCP</td>
<td align="left">Inbound</td>
<td align="left">6443*</td>
<td align="left">Kubernetes API server</td>
</tr>

<tr>
<td align="left">TCP</td>
<td align="left">Inbound</td>
<td align="left">2379-2380</td>
<td align="left">etcd server client API</td>
</tr>

<tr>
<td align="left">TCP</td>
<td align="left">Inbound</td>
<td align="left">10250</td>
<td align="left">Kubelet API</td>
</tr>

<tr>
<td align="left">TCP</td>
<td align="left">Inbound</td>
<td align="left">10251</td>
<td align="left">kube-scheduler</td>
</tr>

<tr>
<td align="left">TCP</td>
<td align="left">Inbound</td>
<td align="left">10252</td>
<td align="left">kube-controller-manager</td>
</tr>

<tr>
<td align="left">TCP</td>
<td align="left">Inbound</td>
<td align="left">10255</td>
<td align="left">Read-only Kubelet API</td>
</tr>

<tr>
<td align="left">UDP</td>
<td align="left">BiDirection</td>
<td align="left">8472</td>
<td align="left">flannel overlay network - vxlan backend</td>
</tr>
</tbody>
</table>

<p><strong>Worker nodes</strong></p>

<table>
<thead>
<tr>
<th align="left">Protocol</th>
<th align="left">Direction</th>
<th align="left">Port Range</th>
<th align="left">Purpose</th>
</tr>
</thead>

<tbody>
<tr>
<td align="left">TCP</td>
<td align="left">Inbound</td>
<td align="left">10250</td>
<td align="left">Kubelet API</td>
</tr>

<tr>
<td align="left">TCP</td>
<td align="left">Inbound</td>
<td align="left">10255</td>
<td align="left">Read-only Kubelet API</td>
</tr>

<tr>
<td align="left">TCP</td>
<td align="left">Inbound</td>
<td align="left">30000-32767</td>
<td align="left">NodePort Services**</td>
</tr>

<tr>
<td align="left">UDP</td>
<td align="left">BiDirection</td>
<td align="left">8472</td>
<td align="left">flannel overlay network - vxlan backend</td>
</tr>
</tbody>
</table>

<h2 id="troubleshoot">Troubleshoot</h2>

<p>Docker is not running</p>

<pre><code>[root@master ~]# docker version
Client:
 Version:	17.12.1-ce
 API version:	1.35
 Go version:	go1.9.4
 Git commit:	7390fc6
 Built:	Tue Feb 27 22:15:20 2018
 OS/Arch:	linux/amd64
Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
</code></pre>

<p>Fix: <code>systemctl start docker</code></p>

<p>K8s default systemd environment in <code>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</code></p>

<h2 id="debug-command">Debug command</h2>

<p><strong>Kubernetes</strong></p>

<ul>
<li><p>[ ] Use busybox images to debug <code>kube-dns</code> by <code>ping</code>, <code>nslookup</code>, <code>traceroute</code> tools. It is equivalent to <code>docker run -it busybox sh</code></p>

<pre><code>kubectl run -i --tty busybox --image=busybox --restart=Never -- sh
</code></pre></li>

<li><p>[ ] <code>journalctl -xe</code></p></li>

<li><p>[ ] Check running container <code>docker ps</code></p></li>

<li><p>[ ] Check the system log file <code>/var/log/messages</code> or <code>journalctl -u kubelet</code></p></li>

<li><p>[ ] <code>kubectl describe pods</code></p></li>

<li><p>[ ] <code>kubectl describe nodes</code></p></li>

<li><p>[ ] <code>kubectl logs</code></p></li>
</ul>

<p><strong>Docker</strong></p>

<ul class="task-list">
<li><label><input type="checkbox" disabled class="task-list-item"> <code>docker inspect &lt;container&gt;</code></label></li>
<li><label><input type="checkbox" disabled class="task-list-item"> <code>docker images -a</code></label></li>
<li><label><input type="checkbox" disabled class="task-list-item"> <code>docker ps</code></label></li>
<li><label><input type="checkbox" disabled class="task-list-item"> <code>docker container ls -a</code></label></li>
</ul>

<p>!!! tips
    Download and export container in RedHat server for offline installation</p>

<pre><code>Enable extras repo
```
yum-config-manager --enable rhel-7-server-extras-rpms
```

Export images to files:

```

docker save python:alpine3.6 &gt; python_alpine3.6.tar
docker save k8s.gcr.io/kube-proxy-amd64:v1.10.4 &gt; kube-proxy-amd64_v1.10.4.tar
docker save k8s.gcr.io/kube-scheduler-amd64:v1.10.4 &gt; kube-controller-manager-amd64_v1.10.4.tar
docker save k8s.gcr.io/kube-apiserver-amd64:v1.10.4 &gt; kube-apiserver-amd64_v1.10.4.tar
docker save k8s.gcr.io/kube-controller-manager-amd64:v1.10.4 &gt; kube-scheduler-amd64_v1.10.4.tar
docker save nginx:1.15.0-alpine &gt; nginx_1.15.0-alpine.tar
docker save k8s.gcr.io/etcd-amd64:3.1.12 &gt; etcd-amd64_3.1.12.tar
docker save quay.io/coreos/flannel:v0.10.0-amd64 &gt; flannel_v0.10.0-amd64.tar
docker save registry:2 &gt; registry_2.tar
docker save k8s.gcr.io/k8s-dns-dnsmasq-nanny-amd64:1.14.8 &gt; k8s-dns-dnsmasq-nanny-amd64_1.14.8.tar
docker save k8s.gcr.io/k8s-dns-sidecar-amd64:1.14.8 &gt; k8s-dns-sidecar-amd64_1.14.8.tar
docker save k8s.gcr.io/k8s-dns-kube-dns-amd64:1.14.8 &gt; k8s-dns-kube-dns-amd64_1.14.8.tar
docker save k8s.gcr.io/pause-amd64:3.1 &gt; pause-amd64_3.1.tar

```
</code></pre>

<h2 id="references">References:</h2>

<ul>
<li><a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">https://github.com/kelseyhightower/kubernetes-the-hard-way</a></li>
<li><a href="https://kubernetes.io/docs/concepts/services-networking/service/">https://kubernetes.io/docs/concepts/services-networking/service/</a></li>
<li><a href="https://github.com/coreos/flannel/blob/master/Documentation/kubernetes.md">https://github.com/coreos/flannel/blob/master/Documentation/kubernetes.md</a></li>
<li><a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/</a></li>
</ul>

<h1 id="install-minikube">Install Minikube</h1>

<h2 id="online-install">Online install</h2>

<ul>
<li>The installation for the minikube is pretty simple and straightforward.</li>

<li><p>Execute the following script</p>

<pre><code class="language-sh">curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 &amp;&amp; chmod +x minikube
curl -Lo kubectl https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl &amp;&amp; chmod +x kubectl

export MINIKUBE_WANTUPDATENOTIFICATION=false
export MINIKUBE_WANTREPORTERRORPROMPT=false
export MINIKUBE_HOME=$HOME
export CHANGE_MINIKUBE_NONE_USER=true
mkdir -p $HOME/.kube
touch $HOME/.kube/config

export KUBECONFIG=$HOME/.kube/config
sudo -E ./minikube start --vm-driver=none

# this for loop waits until kubectl can access the api server that Minikube has created
for i in {1..150}; do # timeout for 5 minutes
./kubectl get po &amp;&gt; /dev/null
if [ $? -ne 1 ]; then
  break
fi
sleep 2
done

# kubectl commands are now able to interact with Minikube cluster
</code></pre></li>
</ul>

<h2 id="offline-install">Offline Install</h2>

<ul>
<li>For the case offline, the binaries minikube, kubeadm, kubelet, and kubectl should be downloaded first. Also, make sure that docker is running, and the necessary docker images are all loaded. The list of docker images can be refer <a href="#step-3-verify-results">here</a></li>
<li>Comment out curl in the script above, and run it.</li>

<li><p>You will notice that it fails to download the kubeadm and kubelet</p>

<pre><code>Downloading kubelet v1.10.0
Downloading kubeadm v1.10.0
* E0615 08:20:33.420600   11477 start.go:252] Error updating cluster:  downloading binaries: downloading kubelet: Error downloading kubelet v1.10.0: failed to download: failed to download to temp file: download failed: 5 error(s) occurred:
* Temporary download error: Get https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kubelet: dial tcp: lookup storage.googleapis.com on [::1]:53: read udp [::1]:53519-&gt;[::1]:53: read: connection refused                                                             * Temporary download error: Get https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kubelet: dial tcp: lookup storage.googleapis.com on [::1]:53: read udp [::1]:40795-&gt;[::1]:53: read: connection refused                                                             * Temporary download error: Get https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kubelet: dial tcp: lookup storage.googleapis.com on [::1]:53: read udp [::1]:56109-&gt;[::1]:53: read: connection refused                                                             * Temporary download error: Get https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kubelet: dial tcp: lookup storage.googleapis.com on [::1]:53: read udp [::1]:50708-&gt;[::1]:53: read: connection refused                                                             * Temporary download error: Get https://storage.googleapis.com/kubernetes-release/release/v1.10.0/bin/linux/amd64/kubelet: dial tcp: lookup storage.googleapis.com on [::1]:53: read udp [::1]:49481-&gt;[::1]:53: read: connection refused
</code></pre></li>

<li><p>To solve this, copy the pre-downloaded kubelet and kubeadm to .minikube/cache/v1.10.0 and re-run it.</p>

<pre><code>$ ls ~/.minikube/cache/v1.10.0/
kubeadm
kubelet
</code></pre></li>

<li><p>Now you should see the message to indicate minikube sucessfully started.</p>

<pre><code>minikube config set WantKubectlDownloadMsg false
Starting local Kubernetes v1.10.0 cluster...
Starting VM...
Getting VM IP address...
Moving files into cluster...
Setting up certs...
Connecting to cluster...
Setting up kubeconfig...
Starting cluster components...
Kubectl is now configured to use the cluster.

WARNING: IT IS RECOMMENDED NOT TO RUN THE NONE DRIVER ON PERSONAL WORKSTATIONS
The 'none' driver will run an insecure kubernetes apiserver as root that may leave the host vulnerable to CSRF attacks
Loading cached images from config file.
</code></pre></li>
</ul>
		</div>
		
<div class="post__tags tags clearfix">
	<svg class="icon icon-tag" width="16" height="16" viewBox="0 0 16 16"><path d="M16 9.5c0 .373-.24.74-.5 1l-5 5c-.275.26-.634.5-1 .5-.373 0-.74-.24-1-.5L1 8a2.853 2.853 0 0 1-.7-1C.113 6.55 0 5.973 0 5.6V1.4C0 1.034.134.669.401.401.67.134 1.034 0 1.4 0h4.2c.373 0 .95.113 1.4.3.45.187.732.432 1 .7l7.5 7.502c.26.274.5.632.5.998zM3.5 5a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3z"/></svg>
	<ul class="tags__list">
		<li class="tags__item"><a class="tags__link btn" href="/tags/cloud/" rel="tag">cloud</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/server/" rel="tag">server</a></li>
	</ul>
</div>
	</article>
</main>

<div class="authorbox clearfix">
	<figure class="authorbox__avatar">
		<img alt="Trong Kha avatar" src="/img/avatar.png" class="avatar" height="90" width="90">
	</figure>
	<div class="authorbox__header">
		<span class="authorbox__name">About Trong Kha</span>
	</div>
	<div class="authorbox__description">
		I am an individual security researcher. I use this corner to share thought on security and stuff that I am working on.
	</div>
</div>

<nav class="post-nav flex">
	<div class="post-nav__item post-nav__item--prev">
		<a class="post-nav__link" href="/post/pretty-json-inside-vim/" rel="prev"><span class="post-nav__caption">«&thinsp;Previous</span><p class="post-nav__post-title">Pretty Json Inside Vim</p></a>
	</div>
	<div class="post-nav__item post-nav__item--next">
		<a class="post-nav__link" href="/post/update-docker/" rel="next"><span class="post-nav__caption">Next&thinsp;»</span><p class="post-nav__post-title">Update all docker images</p></a>
	</div>
</nav>


			</div>
			<aside class="sidebar"><div class="widget-search widget">
	<form class="widget-search__form" role="search" method="get" action="https://google.com/search">
		<label>
			<input class="widget-search__field" type="search" placeholder="SEARCH..." value="" name="q" aria-label="SEARCH...">
		</label>
		<input class="widget-search__submit" type="submit" value="Search">
		<input type="hidden" name="sitesearch" value="/" />
	</form>
</div>
<div class="widget-recent widget">
	<h4 class="widget__title">Recent Posts</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="/post/update-docker/">Update all docker images</a></li>
			<li class="widget__item"><a class="widget__link" href="/post/install-kubernetes/">Install bare-metal kubernetes cluster</a></li>
			<li class="widget__item"><a class="widget__link" href="/post/pretty-json-inside-vim/">Pretty Json Inside Vim</a></li>
			<li class="widget__item"><a class="widget__link" href="/post/offline-install-tips/">Useful commands for offline installation</a></li>
		</ul>
	</div>
</div>
<div class="widget-categories widget">
	<h4 class="widget__title">Categories</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="/categories/container">Container</a></li>
			<li class="widget__item"><a class="widget__link" href="/categories/development">Development</a></li>
			<li class="widget__item"><a class="widget__link" href="/categories/devops">Devops</a></li>
			<li class="widget__item"><a class="widget__link" href="/categories/tip">Tip</a></li>
			<li class="widget__item"><a class="widget__link" href="/categories/tips">Tips</a></li>
		</ul>
	</div>
</div>
<div class="widget-taglist widget">
	<h4 class="widget__title">Tags</h4>
	<div class="widget__content">
		<a class="widget-taglist__link widget__link btn" href="/tags/cloud" title="Cloud">Cloud</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/server" title="Server">Server</a>
	</div>
</div>
</aside>
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2019 Kha&#39;s.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script></body>
</html>